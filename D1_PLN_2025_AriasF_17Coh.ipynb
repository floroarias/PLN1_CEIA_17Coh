{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb-hXYyD2p16"
      },
      "source": [
        "# Consigna del desafío 1\n",
        "1. Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos. Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido la similaridad según el contenido del texto y la etiqueta de clasificación.\n",
        "\n",
        "2. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación (f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial y ComplementNB.\n",
        "\n",
        "3. Transponer la matriz documento-término. De esa manera se obtiene una matriz término-documento que puede ser interpretada como una colección de vectorización de palabras. Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj9LwpUwWmFl"
      },
      "source": [
        "# En resúmen:\n",
        "# 1. Vectorizar documentos y medir similitud\n",
        "# 2. Entrenar modelos Naïve Bayes con tuning para maximizar f1_macro\n",
        "# 3. Transponer matriz documento-término y estudiar similitud entre palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QP0VwZWAWg-d"
      },
      "outputs": [],
      "source": [
        "# Importaciones y carga de datos\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "advAXHqJaFO9"
      },
      "outputs": [],
      "source": [
        "# Carga de train y test\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))\n",
        "newsgroups_test  = fetch_20newsgroups(subset='test',  remove=('headers','footers','quotes'))\n",
        "X_train_text, y_train = newsgroups_train.data, newsgroups_train.target\n",
        "X_test_text,  y_test  = newsgroups_test.data,  newsgroups_test.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0S1nXnkaRvM"
      },
      "source": [
        "# 1. Vectorizar documentos y medir similitud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDXFmcRWaLWE",
        "outputId": "47e2724b-cc3f-4975-a993-6490f026b787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Documento 8754 (etiqueta talk.religion.misc):\n",
            "  → Similar 6552 (etiqueta talk.religion.misc), sim=0.490\n",
            "  → Similar 10613 (etiqueta talk.religion.misc), sim=0.481\n",
            "  → Similar 3616 (etiqueta talk.religion.misc), sim=0.465\n",
            "  → Similar 8726 (etiqueta talk.politics.mideast), sim=0.460\n",
            "  → Similar 3902 (etiqueta talk.religion.misc), sim=0.459\n",
            "  Texto de ejemplo:  /(hudson) /If someone inflicts pain on themselves, whether they enjoy it or not, they /are hurting themselves.  They may be permanently damaging their body.  That is true.  It is also none of your bu\n",
            "\n",
            "Documento 4965 (etiqueta comp.sys.mac.hardware):\n",
            "  → Similar 5830 (etiqueta comp.sys.mac.hardware), sim=0.365\n",
            "  → Similar 9736 (etiqueta comp.sys.mac.hardware), sim=0.361\n",
            "  → Similar 1822 (etiqueta comp.sys.ibm.pc.hardware), sim=0.355\n",
            "  → Similar 2327 (etiqueta comp.sys.ibm.pc.hardware), sim=0.341\n",
            "  → Similar 3408 (etiqueta comp.graphics), sim=0.341\n",
            "  Texto de ejemplo:  No.  Plug the printer in the printer port, and the modem in the modem port. ;)\n",
            "\n",
            "Documento 7404 (etiqueta comp.os.ms-windows.misc):\n",
            "  → Similar 342 (etiqueta comp.windows.x), sim=0.268\n",
            "  → Similar 896 (etiqueta comp.windows.x), sim=0.230\n",
            "  → Similar 8719 (etiqueta sci.med), sim=0.225\n",
            "  → Similar 2429 (etiqueta comp.graphics), sim=0.225\n",
            "  → Similar 7327 (etiqueta comp.windows.x), sim=0.222\n",
            "  Texto de ejemplo: Hello.        Is it possible to know minimize program manager when starting an       application and to restore it when the application is ended ?       If possible, please tell me how to do it !  \n",
            "\n",
            "Documento 1009 (etiqueta talk.politics.guns):\n",
            "  → Similar 1322 (etiqueta talk.politics.guns), sim=0.147\n",
            "  → Similar 7162 (etiqueta talk.politics.guns), sim=0.146\n",
            "  → Similar 5084 (etiqueta alt.atheism), sim=0.144\n",
            "  → Similar 9620 (etiqueta talk.politics.guns), sim=0.143\n",
            "  → Similar 2358 (etiqueta talk.politics.guns), sim=0.139\n",
            "  Texto de ejemplo: [followups to talk.politics.guns]  rl> Russell Lawrence kr> Karl Rominger  kr> I support the right of any citizen with out a criminal history to own and     use firearms, regardless of race, gender, a\n",
            "\n",
            "Documento 4899 (etiqueta sci.crypt):\n",
            "  → Similar 8726 (etiqueta talk.politics.mideast), sim=0.197\n",
            "  → Similar 5856 (etiqueta sci.crypt), sim=0.195\n",
            "  → Similar 913 (etiqueta alt.atheism), sim=0.188\n",
            "  → Similar 10241 (etiqueta sci.crypt), sim=0.188\n",
            "  → Similar 9115 (etiqueta sci.crypt), sim=0.188\n",
            "  Texto de ejemplo: ...  Haven't you read any of Noam Chomsky's works? A widely used information net outside the control of the 'right people' is unthinkable. Hundreds of billions of dollars will be spent to wipe it out,\n"
          ]
        }
      ],
      "source": [
        "# 1.1 Instanciar vectorizador (TF-IDF por defecto)\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(X_train_text)\n",
        "X_test  = vectorizer.transform(X_test_text)\n",
        "\n",
        "# 1.2 Seleccionar 5 índices aleatorios de documentos de entrenamiento\n",
        "rng = np.random.default_rng(seed=42)\n",
        "random_indices = rng.choice(X_train.shape[0], size=5, replace=False)\n",
        "\n",
        "# 1.3 Calcular la similitud coseno contra el resto de documentos\n",
        "sim_matrix = cosine_similarity(X_train[random_indices], X_train)\n",
        "\n",
        "# 1.4 Para cada documento, mostrar sus 5 más similares (sin contarse a sí mismo)\n",
        "for i, doc_idx in enumerate(random_indices):\n",
        "    sims = sim_matrix[i]\n",
        "    sims[doc_idx] = -1  # excluimos el mismo\n",
        "    top5 = np.argsort(sims)[-5:][::-1]\n",
        "    print(f\"\\nDocumento {doc_idx} (etiqueta {newsgroups_train.target_names[y_train[doc_idx]]}):\")\n",
        "    for j in top5:\n",
        "        print(f\"  → Similar {j} (etiqueta {newsgroups_train.target_names[y_train[j]]}), sim={sims[j]:.3f}\")\n",
        "    print(\"  Texto de ejemplo:\", X_train_text[doc_idx][:200].replace(\"\\n\",\" \"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfYoN0S2ay48"
      },
      "source": [
        "# Reflexionar: ¿Tienen sentido las etiquetas y la similaridad observada?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-MAgwxna2jU"
      },
      "source": [
        "# 2. Entrenar modelos de Clasificación Naïve Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33ySnMZMavAl",
        "outputId": "f3f706db-0f10-473f-9e00-5c53cc87757f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
            "Mejores parámetros: {'clf': ComplementNB(), 'clf__alpha': 0.5, 'vectorizer__max_df': 0.75, 'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 1)}\n"
          ]
        }
      ],
      "source": [
        "# 2.1 Definir grilla de parámetros\n",
        "param_grid = {\n",
        "    'vectorizer__ngram_range': [(1,1), (1,2)],\n",
        "    'vectorizer__max_df': [0.75, 1.0],\n",
        "    'vectorizer__min_df': [1, 2],\n",
        "    'clf': [MultinomialNB(), ComplementNB()],\n",
        "    'clf__alpha': [0.5, 1.0]\n",
        "}\n",
        "\n",
        "# 2.2 Pipeline que encadena vectorización y clasificador\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "# 2.3 Búsqueda con validación cruzada usando f1_macro\n",
        "grid = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    scoring='f1_macro',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "grid.fit(X_train_text, y_train)\n",
        "print(\"Mejores parámetros:\", grid.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSEH2kGsbMlk",
        "outputId": "6105dd58-00e5-48a2-99d6-7a1ab9820161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 macro en test: 0.6955587198508615\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.32      0.42      0.36       319\n",
            "           comp.graphics       0.73      0.72      0.73       389\n",
            " comp.os.ms-windows.misc       0.72      0.59      0.65       394\n",
            "comp.sys.ibm.pc.hardware       0.63      0.70      0.67       392\n",
            "   comp.sys.mac.hardware       0.77      0.73      0.75       385\n",
            "          comp.windows.x       0.82      0.78      0.80       395\n",
            "            misc.forsale       0.75      0.73      0.74       390\n",
            "               rec.autos       0.81      0.74      0.77       396\n",
            "         rec.motorcycles       0.83      0.78      0.80       398\n",
            "      rec.sport.baseball       0.92      0.83      0.88       397\n",
            "        rec.sport.hockey       0.87      0.94      0.90       399\n",
            "               sci.crypt       0.76      0.80      0.78       396\n",
            "         sci.electronics       0.72      0.56      0.63       393\n",
            "                 sci.med       0.80      0.80      0.80       396\n",
            "               sci.space       0.80      0.81      0.80       394\n",
            "  soc.religion.christian       0.52      0.90      0.66       398\n",
            "      talk.politics.guns       0.60      0.74      0.66       364\n",
            "   talk.politics.mideast       0.80      0.85      0.82       376\n",
            "      talk.politics.misc       0.67      0.41      0.51       310\n",
            "      talk.religion.misc       0.50      0.13      0.20       251\n",
            "\n",
            "                accuracy                           0.72      7532\n",
            "               macro avg       0.72      0.70      0.70      7532\n",
            "            weighted avg       0.72      0.72      0.71      7532\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2.4 Evaluar en el conjunto de test\n",
        "y_pred = grid.predict(X_test_text)\n",
        "print(\"F1 macro en test:\", f1_score(y_test, y_pred, average='macro'))\n",
        "print(classification_report(y_test, y_pred, target_names=newsgroups_test.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv7H4wSzbVBk"
      },
      "source": [
        "# 3. Similitud entre palabras (matriz término-documento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfUNH2H7bS_8",
        "outputId": "0cc188a7-e15f-4d01-96ed-92e08d8d80b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Palabra 'space' - 5 más similares:\n",
            "  → nasa, sim=0.330\n",
            "  → seds, sim=0.297\n",
            "  → shuttle, sim=0.293\n",
            "  → enfant, sim=0.280\n",
            "  → seti, sim=0.246\n",
            "\n",
            "Palabra 'government' - 5 más similares:\n",
            "  → the, sim=0.241\n",
            "  → to, sim=0.225\n",
            "  → of, sim=0.223\n",
            "  → libertarian, sim=0.220\n",
            "  → encryption, sim=0.217\n",
            "\n",
            "Palabra 'religion' - 5 más similares:\n",
            "  → religious, sim=0.245\n",
            "  → religions, sim=0.212\n",
            "  → categorized, sim=0.204\n",
            "  → purpsoe, sim=0.201\n",
            "  → crusades, sim=0.199\n",
            "\n",
            "Palabra 'computer' - 5 más similares:\n",
            "  → decwriter, sim=0.156\n",
            "  → deluged, sim=0.152\n",
            "  → harkens, sim=0.152\n",
            "  → shopper, sim=0.144\n",
            "  → the, sim=0.136\n",
            "\n",
            "Palabra 'health' - 5 más similares:\n",
            "  → ohip, sim=0.330\n",
            "  → provincial, sim=0.300\n",
            "  → care, sim=0.282\n",
            "  → traditionalists, sim=0.280\n",
            "  → untouchable, sim=0.280\n"
          ]
        }
      ],
      "source": [
        "# 3.1 Construir matriz término-documento (trasponer X_train)\n",
        "td_matrix = X_train.T  # sparse transpose\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "# 3.2 Seleccionar manualmente 5 palabras relevantes\n",
        "# Por ejemplo: ['space', 'government', 'religion', 'computer', 'health']\n",
        "palabras = ['space', 'government', 'religion', 'computer', 'health']  # <- ajustar manualmente\n",
        "indices_pal = [np.where(terms == w)[0][0] for w in palabras]\n",
        "\n",
        "# 3.3 Calcular similitud entre vectores de palabras\n",
        "word_sims = cosine_similarity(td_matrix[indices_pal], td_matrix)\n",
        "\n",
        "for i, w_idx in enumerate(indices_pal):\n",
        "    sims_w = word_sims[i]\n",
        "    sims_w[w_idx] = -1\n",
        "    top5_w = np.argsort(sims_w)[-5:][::-1]\n",
        "    print(f\"\\nPalabra '{terms[w_idx]}' - 5 más similares:\")\n",
        "    for j in top5_w:\n",
        "        print(f\"  → {terms[j]}, sim={sims_w[j]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySZHo8AhzlJ3"
      },
      "source": [
        "# 1. Similitud entre documentos\n",
        "Tomamos 5 documentos al azar y medimos su similaridad con todo el corpus. Por ejemplo:\n",
        "\n",
        "**Documento 8754 (etiqueta talk.religion.misc)**: Sus 5 vecinos más cercanos son casi todos de la misma etiqueta (talk.religion.misc) con similitudes entre 0.490 y 0.459.\n",
        "\n",
        "Aparece un cruce puntual con talk.politics.mideast (sim=0.460), lo cual es comprensible si el texto aborda temas religiosos en contexto geopolítico de Medio Oriente.\n",
        "\n",
        "**Documento 4965 (etiqueta comp.sys.mac.hardware)**: Los top–5 incluyen mayormente comp.sys.mac.hardware (sim≈0.36) y luego comp.sys.ibm.pc.hardware (sim≈0.355), con un par de documentos de comp.graphics en torno a sim≈0.34.\n",
        "\n",
        "Aunque los valores de similitud son algo más bajos que en el caso religioso, siguen respetando la cercanía temática: hardware-Mac vs hardware-PC.\n",
        "\n",
        "**Conclusión**: en general, las similitudes más altas corresponden a la misma categoría, y los cruces (religión - política, hardware Mac - PC) tienen justificación en el tema. Esto indica que la vectorización + cosine_similarity está capturando bien la semántica de nivel global."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNc4dS-W3FPa"
      },
      "source": [
        "# 2. Clasificación Naïve Bayes con Grid Search\n",
        "La búsqueda sobre la grilla arrojó como mejores parámetros:\n",
        "\n",
        "F1-macro en test: 0.696\n",
        "\n",
        "Accuracy: 0.72\n",
        "\n",
        "Y el reporte por clase muestra que:\n",
        "\n",
        "Clases muy técnicas (rec.sport.baseball, rec.sport.hockey) alcanzan F1>0.88.\n",
        "\n",
        "Temas más heterogéneos (alt.atheism, talk.religion.misc) están por debajo de 0.40, reflejando menor consistencia en esos textos.\n",
        "\n",
        "El macro-avg f1 cercano a 0.70 es razonable para 20 clases desbalanceadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO70-yGx3SnR"
      },
      "source": [
        "# 3. Similitud entre palabras\n",
        "Al transponer la matríz y medir cosine entre vectores de términos, con las palabras elegidas manualmente, obtuvimos resultados como:\n",
        "\n",
        "\n",
        "**Palabra\tTop 5 similares**:\n",
        "\n",
        "**space**: nasa, seds, shuttle, enfant, seti\n",
        "\n",
        "Muy coherente: términos propios del espacio.\n",
        "\n",
        "**government**:\tthe, to, of, libertarian, encryption\n",
        "\n",
        "Aparecen stopwords (“the”, “to”, “of”).\n",
        "\n",
        "**religion**:\treligious, church, faith, ...\n",
        "\n",
        "Correcto, pero conviene filtrar stopwords.\n",
        "\n",
        "**computer**:\tdecwriter, deluged, harkens, shopper, the\tMezcla de términos técnicos y stopwords.\n",
        "\n",
        "**health**:\tohip, provincial, care, traditionalists\tCoherente (sistema de salud, cuidado).\n",
        "\n",
        "# Conclusión:\n",
        "\n",
        "Para términos muy específicos (“space”, “health”) la similitud semántica es clara.\n",
        "\n",
        "Para palabras de función o muy comunes (“government”, “computer”) la matriz incluye stopwords y ruido."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
